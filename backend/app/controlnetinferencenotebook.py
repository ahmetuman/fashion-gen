# -*- coding: utf-8 -*-
"""ControlNetInferenceNotebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Az_o8qsPAg3VS5Hg6lh0sJrPJqlLfht8
"""

from google.colab import drive
drive.mount('/content/drive', force_remount= True)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %cd /content/
# !git clone https://github.com/huggingface/diffusers
# !pip install -q git+https://github.com/huggingface/diffusers
# !pip install -q accelerate==0.12.0
# !pip install -q OmegaConf
# !pip install -q wget
# !pip install -q pytorch_lightning
# !pip install -q huggingface_hub
# !pip install -U -q --no-cache-dir gdown
# !wget https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Deps
# !mv Deps Deps.7z
# !7z x Deps.7z
# !cp -r /content/usr/local/lib/python3.7/dist-packages /usr/local/lib/python3.7/
# !rm Deps.7z
# !rm -r /content/usr
# !sed -i 's@else prefix + ": "@else prefix + ""@g' /usr/local/lib/python3.7/dist-packages/tqdm/std.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/diffusers/examples/controlnet/

!pip install -r requirements.txt

# Commented out IPython magic to ensure Python compatibility.
from subprocess import getoutput
from IPython.display import HTML
from IPython.display import clear_output
import wget
import time

s = getoutput('nvidia-smi')
if 'T4' in s:
  gpu = 'T4'
elif 'P100' in s:
  gpu = 'P100'
elif 'V100' in s:
  gpu = 'V100'
elif 'A100' in s:
  gpu = 'A100'

while True:
    try:
        gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'
        break
    except:
        pass
    print('[1;31mit seems that your GPU is not supported at the moment')
    time.sleep(5)

if (gpu=='T4'):
#   %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl

elif (gpu=='P100'):
#   %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl

elif (gpu=='V100'):
#   %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl

elif (gpu=='A100'):
#   %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl

clear_output()
print('[1;32mDONE !')

!pip install datasets

from datasets import load_dataset
from huggingface_hub import notebook_login

notebook_login()

import json
with open('/content/test_yeni/test_prompts.json', 'r') as f:
    data = json.load(f)
def extract_object(index):
    if isinstance(data, list) and len(data) > 0:
        return data[index]
    elif isinstance(data, dict):
        return data
    else:
        return "File does not contain a list or dictionary"

import importlib.util
import sys
spec = importlib.util.spec_from_file_location("stable_diffusion_controlnet_inpaint", "/content/diffusers/examples/community/stable_diffusion_controlnet_inpaint.py")
stable_diffusion_controlnet_inpaint = importlib.util.module_from_spec(spec)
sys.modules["stable_diffusion_controlnet_inpaint"] = stable_diffusion_controlnet_inpaint
spec.loader.exec_module(stable_diffusion_controlnet_inpaint)

# Commented out IPython magic to ensure Python compatibility.
#@markdown # xformers

from subprocess import getoutput
from IPython.display import HTML
from IPython.display import clear_output
import wget
import time

s = getoutput('nvidia-smi')
if 'T4' in s:
  gpu = 'T4'
elif 'P100' in s:
  gpu = 'P100'
elif 'V100' in s:
  gpu = 'V100'
elif 'A100' in s:
  gpu = 'A100'

while True:
    try:
        gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'
        break
    except:
        pass
    print('[1;31mit seems that your GPU is not supported at the moment')
    time.sleep(5)

if (gpu=='T4'):
#   %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl

elif (gpu=='P100'):
#   %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl

elif (gpu=='V100'):
#   %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl

elif (gpu=='A100'):
#   %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl

clear_output()
print('[1;32mDONE !')

import shutil
shutil.copy2('/content/drive/MyDrive/Dataset/vitonHD DATASET/test/agnostic-v3.2/00254_00.jpg', '/content')

import shutil
shutil.copy2('/content/drive/MyDrive/Dataset/vitonHD DATASET/test/agnostic-mask/00254_00_mask.png', '/content')

import shutil
shutil.copy2('/content/drive/MyDrive/Dataset/vitonHD DATASET/test/image-parse-v3/00254_00.png', '/content')

from PIL import Image
import os

def pad_to_square_with_edge_fill(img):
    """
    Pads the image left/right with its edge pixels to make it square (width == height).
    Only pads left/right, not top/bottom.
    Returns the squared image.
    """
    width, height = img.size
    if width == height:
        return img

    # Pad left/right if needed
    if width < height:
        pad_total = height - width
        pad_left = pad_total // 2
        pad_right = pad_total - pad_left

        # Get left and right columns as images
        left_column = img.crop((0, 0, 1, height))
        right_column = img.crop((width - 1, 0, width, height))

        # Create new image
        new_img = Image.new("RGB", (height, height))
        # Paste left padding
        for x in range(pad_left):
            new_img.paste(left_column, (x, 0))
        # Paste original image
        new_img.paste(img, (pad_left, 0))
        # Paste right padding
        for x in range(pad_left + width, height):
            new_img.paste(right_column, (x, 0))
        return new_img
    else:
        # If width > height, crop center square (no top/bottom padding)
        left = (width - height) // 2
        return img.crop((left, 0, left + height, height))

def resize_and_pad(image_path, output_path, target_size=(512, 512)):
    """
    Pads the image left/right to make it square using edge pixel fill, then downsamples to target_size.
    """
    img = Image.open(image_path).convert("RGB")
    img = pad_to_square_with_edge_fill(img)
    img = img.resize(target_size, Image.LANCZOS)
    img.save(output_path)

def process_images(input_folder, output_folder, target_size=(512, 512)):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for file_name in os.listdir(input_folder):
        if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):
            input_path = os.path.join(input_folder, file_name)
            output_path = os.path.join(output_folder, file_name)
            resize_and_pad(input_path, output_path, target_size)
            print(f"Processed: {file_name}")

input_folder = "/content/"
output_folder = "/content/resized"
process_images(input_folder, output_folder)

!pip install -U datasets

import numpy as np
import torch
from PIL import Image
from transformers import AutoImageProcessor, UperNetForSemanticSegmentation
from diffusers import ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
from huggingface_hub import snapshot_download

#controlnet = ControlNetModel.from_pretrained("menevseyup/output", torch_dtype=torch.float16)

# Step 2: Load your ControlNet model
controlnet = ControlNetModel.from_pretrained("menevseyup/cnet-inpaint-15-05-2025", torch_dtype=torch.float16)

# Step 3: Load the pipeline from the downloaded model path
pipe = stable_diffusion_controlnet_inpaint.StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "botp/stable-diffusion-v1-5-inpainting",
    controlnet=controlnet,
    safety_checker=None,
    torch_dtype=torch.float16
).to("cuda")

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

image = load_image(
        "/content/resized/00249_00.jpg"
)
mask_image = load_image(
        "/content/resized/00249_00_mask.png"
)
controlnet_conditioning_image = load_image(
        "/content/resized/00249_00.png"
)
generator = torch.Generator(device="cuda").manual_seed(42)
result = pipe(
    prompt="a white, solid sweater with a relaxed fit, white and long sleeves, and untucked style. The sweater is hip-length, made of cotton, and has a crew neckline.",
    image=image,
    mask_image=mask_image,
    controlnet_conditioning_image=controlnet_conditioning_image,
    num_inference_steps=30,
    guidance_scale=7.5,
    generator = generator
).images[0]
result.save("/content/image_result_599.png")

import os
import zipfile
from google.colab import drive

# Count the number of items in the directory
image_dir = "/content/image_outputs/image"
num_items = len([name for name in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, name))])
print(f"Number of items in {image_dir}: {num_items}")

# Zip the contents of the directory
zip_filename = "/content/images.zip"
with zipfile.ZipFile(zip_filename, 'w') as zipf:
  for root, _, files in os.walk(image_dir):
    for file in files:
      zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(image_dir, '..')))
# Get the size of the zip file in bytes
zip_size = os.path.getsize(zip_filename)

# Print the size in a human-readable format
print(f"Zip file size: {zip_size / (1024 * 1024):.2f} MB")  # Convert bytes to MB

# Move the zip file to Google Drive
drive.mount('/content/drive', force_remount=True)  # Ensure Google Drive is mounted

import shutil
destination_path = "/content/drive/MyDrive/images.zip" # Specify your desired destination path
shutil.move(zip_filename, destination_path)

print(f"Zip file moved to: {destination_path}")

"""## Eren"""

import numpy as np
import torch
from PIL import Image
from transformers import AutoImageProcessor, UperNetForSemanticSegmentation
from diffusers import ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
from huggingface_hub import snapshot_download

#controlnet = ControlNetModel.from_pretrained("menevseyup/output", torch_dtype=torch.float16)

local_model_path = snapshot_download(
    repo_id="menevseyup/sd-inpaint-trained",
    cache_dir="/content/huggingface",  # or any path you prefer
    force_download=False  # optional; remove if you don't need to re-download
)

# Step 2: Load your ControlNet model
controlnet = ControlNetModel.from_pretrained("merensoykok/output", torch_dtype=torch.float16)

# Step 3: Load the pipeline from the downloaded model path
pipe = stable_diffusion_controlnet_inpaint.StableDiffusionControlNetInpaintPipeline.from_pretrained(
    local_model_path,
    controlnet=controlnet,
    safety_checker=None,
    torch_dtype=torch.float16
).to("cuda")


pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)


def test_data(image, mask_image, controlnet_conditioning_image, prompt, save_name):
    result = pipe(
        prompt=prompt,
        image=image,
        mask_image=mask_image,
        controlnet_conditioning_image=controlnet_conditioning_image,
        num_inference_steps=30,
        guidance_scale=7.5,
    ).images[0]

    result.save(save_name)


prompt = "The model is wearing a black, solid blouse with a relaxed fit, three-quarter sleeves, and an untucked style. The blouse is medium in length, likely made of polyester, and has a scoop neckline with a small keyhole detail."
source = load_image("/content/drive/MyDrive/Dataset/vitonHD DATASET/test/agnostic-v3.2/00006_00.jpg") # image
mask_image = load_image("/content/drive/MyDrive/Dataset/vitonHD DATASET/test/agnostic-mask/00006_00_mask.png") # controlnet_conditioning_image ve mask_image
target = load_image("/content/drive/MyDrive/Dataset/vitonHD DATASET/test/image/00006_00.jpg") # metrik kÄ±yaslarken kullan

save_name = "target_result"

test_data(source, mask_image, mask_image, prompt, save_name)



!unzip "/content/drive/MyDrive/Dataset/test - resized.zip" -d "/content/drive/MyDrive/Dataset/VitonHD Dataset - Resized"

import numpy as np
import torch
from PIL import Image
from transformers import AutoImageProcessor, UperNetForSemanticSegmentation
from diffusers import ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
from huggingface_hub import snapshot_download
from diffusers import StableDiffusionControlNetInpaintPipeline, StableDiffusionInpaintPipeline



# Step 2: Load your ControlNet model
controlnet = ControlNetModel.from_pretrained("merensoykok/output", torch_dtype=torch.float16)

# Step 3: Load the pipeline from the downloaded model path
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-inpainting",
    controlnet=controlnet,
    safety_checker=None,
    torch_dtype=torch.float16
).to("cuda")


pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)



prompt = "The model is wearing a black, solid blouse with a relaxed fit, three-quarter sleeves, and an untucked style. The blouse is medium in length, likely made of polyester, and has a scoop neckline with a small keyhole detail."
source = load_image("/content/00006_00.jpg") # image
mask_image = load_image("/content/00006_00_mask.png") # controlnet_conditioning_image ve mask_image
#target = load_image("00006_00.jpg") # metrik kÄ±yaslarken kullan

source = source.resize((512, 512))
mask_image = mask_image.resize((512, 512))

save_name = "target_result"

result = pipe(
    prompt=prompt,
    image=source,
    mask_image=mask_image,
    controlnet_conditioning_image=mask_image,
    num_inference_steps=30,
    guidance_scale=7.5,
).images[0]

result.save(save_name)

from diffusers import StableDiffusionControlNetInpaintPipeline, StableDiffusionInpaintPipeline


pipe = StableDiffusionInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-inpainting",
    safety_checker=None,
    torch_dtype=torch.float16
).to("cuda")

!cp '/content/inpainting_test.py' '/content/diffusers/examples/controlnet/'

!python3 inpainting_test.py --dataset menevseyup/test-cnet-seg-dataset --split test --output-dir /content/output

import zipfile
import os

def zip_folder(folder_path, output_path):
  with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for root, _, files in os.walk(folder_path):
      for file in files:
        file_path = os.path.join(root, file)
        arcname = os.path.relpath(file_path, folder_path)
        zipf.write(file_path, arcname)
# Example usage
zip_folder('/content/output', '/content/drive/MyDrive/Dataset/vitonHD DATASET/test/all_output_images_17-05-2025.zip')